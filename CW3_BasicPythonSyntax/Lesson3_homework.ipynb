{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1316abd3",
   "metadata": {},
   "source": [
    "1. Напишите программу, которая подсчитывает в строке количeство гласных и согласных букв.\n",
    "\n",
    "#Write a program that counts the number of vowels and consonants in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примените эту программу к строке\n",
    "'To do this, we will calculate the K-core of our graph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "227b97d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of vowels: 16\n",
      "The number of consonants: 25\n"
     ]
    }
   ],
   "source": [
    "# Number 1\n",
    "#we enter our variables and string,set 0 value to vowels and consonants.\n",
    "\n",
    "str = 'To do this, we will calculate the K-core of our graph'\n",
    "vowels = 0\n",
    "consonants = 0\n",
    "\n",
    "\n",
    "#define the function lower to return a lowercase string for counting.\n",
    "str.lower()\n",
    "\n",
    "# loop with condition: if i equals to vowels letters, then the programm should add it (+1) to our variable vowels.\n",
    "#otherwise the programm will add (+1) to consonants.\n",
    "\n",
    "for i in str:\n",
    "# ОБЯЗАТЕЛЬНО ПИШЕМ i.isalpha(), ЧТОБЫ ПОСЧИТАТЬ ТОЛЬКО БУКВЫ. ИНАЧЕ БУДУТ УЧИТЫВАТЬСЯ ТАКЖЕ ЗАПЯТЫЕ,ПРОБЕЛЫ И Т.Д.\n",
    "    if i.isalpha():\n",
    "        \n",
    "        if (i == 'a' or i == 'i' or i == 'e' or i == 'o' or i == 'u'):\n",
    "            vowels +=1;\n",
    "        else:\n",
    "            consonants +=1;\n",
    "        \n",
    "print ('The number of vowels:', vowels)\n",
    "print ('The number of consonants:', consonants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "57f334ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of vowels: 16\n",
      "The number of consonants: 37\n"
     ]
    }
   ],
   "source": [
    "# Number 2\n",
    "#we enter our variables and string,set 0 value to vowels and consonants.\n",
    "\n",
    "str = 'To do this, we will calculate the K-core of our graph'\n",
    "vowels = 0\n",
    "consonants = 0\n",
    "\n",
    "#define the function lower to return a lowercase string for counting.\n",
    "str = str.lower()\n",
    "\n",
    "# Loop for via range (from 0 till the length of our string)\n",
    "#check if character is a vowel:(если объект в нашей строке гласная, значит мы добавим +1 к нашей переменной vowals)\n",
    "for i in range (0,len(str)):\n",
    "   # if i.isalpha():\n",
    "        if str[i] in ('a','e','i','o','u'):\n",
    "            vowels = vowels + 1\n",
    "        else:\n",
    "            consonants +=1;\n",
    "        \n",
    "print ('The number of vowels:', vowels)\n",
    "print ('The number of consonants:', consonants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d8949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72eef77e",
   "metadata": {},
   "source": [
    "2. Напишите программу, которая подсчитывает:\n",
    "    - количество предложений в тексте\n",
    "    - количество слов в тексте\n",
    "    - количество уникальных слов и количество использований каждого слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d2ea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Программу нужно написать для кокретного текста, переменная с текстом определена ниже. \n",
    "# символ \\ это служебный символ переноса строки, в самом тексте его нет.\n",
    "text = \"\"\"We first preprocess the data. We will give a summary of the most important parts,\\\n",
    "but please see our preprocessing code in our GitHub repo for more details.\\\n",
    "The Spotify dataset has 1 million playlists, over 2 million unique songs,\\\n",
    "and over 66 million edges linking playlists to songs. This is very large. We will cut\\\n",
    "down the dataset so that it can be trained on Colab in a reasonable amount of time.\\\n",
    "To do this, we will calculate the K-core of our graph. The K-core of a graph G is\\\n",
    "the largest possible connected subgraph of G, in which every node has a degree of at\\\n",
    "least K. This will give us the largest subgraph in which each playlist contains at least\\\n",
    "K songs, and each song is in at least K playlists.\\\n",
    "This has two benefits. Firstly, it will reduce the size of our dataset.\\\n",
    "Secondly, it will eliminate rare/unpopular songs, and small playlists.\\\n",
    "The remaining playlists/songs will all be relatively information-rich, which will\\\n",
    "make the learning process a bit easier, which is ideal for the purposes of this article.\"\"\"\n",
    "\n",
    "# 1. number of sentences in the text:\n",
    "#мы считаем по точкам количество предложений.\n",
    "dot = 0\n",
    "for i in text:\n",
    "    if i == '.':\n",
    "        dot +=1\n",
    "        \n",
    "print (dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e04ddb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are : 179 words\n"
     ]
    }
   ],
   "source": [
    "# 2.number of words in the text:\n",
    "text = \"\"\"We first preprocess the data. We will give a summary of the most important parts,\\\n",
    " but please see our preprocessing code in our GitHub repo for more details.\\\n",
    " The Spotify dataset has 1 million playlists, over 2 million unique songs,\\\n",
    " and over 66 million edges linking playlists to songs. This is very large. We will cut\\\n",
    " down the dataset so that it can be trained on Colab in a reasonable amount of time.\\\n",
    " To do this, we will calculate the K-core of our graph. The K-core of a graph G is\\\n",
    " the largest possible connected subgraph of G, in which every node has a degree of at\\\n",
    " least K. This will give us the largest subgraph in which each playlist contains at least\\\n",
    "K songs, and each song is in at least K playlists.\\\n",
    "This has two benefits. Firstly, it will reduce the size of our dataset.\\\n",
    "Secondly, it will eliminate rare/unpopular songs, and small playlists.\\\n",
    "The remaining playlists/songs will all be relatively information-rich, which will\\\n",
    "make the learning process a bit easier, which is ideal for the purposes of this article.\"\"\"\n",
    "\n",
    "new_text=text.replace('/',',')\n",
    "new_text1=new_text.replace(',',' ')\n",
    "new_text2=new_text1.replace('.',' ')\n",
    "\n",
    "result = len(new_text2.split())\n",
    "#result = new_text2.split()\n",
    "\n",
    "print('There are :', result, 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "448c5e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of each word: {'we': 4, 'first': 1, 'preprocess': 1, 'the': 12, 'data': 1, 'will': 8, 'give': 2, 'a': 5, 'summary': 1, 'of': 8, 'most': 1, 'important': 1, 'parts': 1, 'but': 1, 'please': 1, 'see': 1, 'our': 4, 'preprocessing': 1, 'code': 1, 'in': 5, 'github': 1, 'repo': 1, 'for': 2, 'more': 1, 'details': 1, 'spotify': 1, 'dataset': 3, 'has': 3, '1': 1, 'million': 3, 'playlists': 5, 'over': 2, '2': 1, 'unique': 1, 'songs': 5, 'and': 3, '66': 1, 'edges': 1, 'linking': 1, 'to': 2, 'this': 5, 'is': 4, 'very': 1, 'large': 1, 'cutdown': 1, 'so': 1, 'that': 1, 'it': 3, 'can': 1, 'be': 2, 'trained': 1, 'on': 1, 'colab': 1, 'reasonable': 1, 'amount': 1, 'time': 1, 'do': 1, 'calculate': 1, 'k-core': 2, 'graph': 2, 'g': 2, 'largest': 2, 'possible': 1, 'connected': 1, 'subgraph': 2, 'which': 4, 'every': 1, 'node': 1, 'degree': 1, 'atleast': 1, 'k': 2, 'us': 1, 'each': 2, 'playlist': 1, 'contains': 1, 'at': 2, 'leastk': 1, 'song': 1, 'least': 1, 'two': 1, 'benefits': 1, 'firstly': 1, 'reduce': 1, 'size': 1, 'secondly': 1, 'eliminate': 1, 'rare': 1, 'unpopular': 1, 'small': 1, 'remaining': 1, 'all': 1, 'relatively': 1, 'information-rich': 1, 'make': 1, 'learning': 1, 'process': 1, 'bit': 1, 'easier': 1, 'ideal': 1, 'purposes': 1, 'article': 1}\n",
      "The number of unique words: 101\n"
     ]
    }
   ],
   "source": [
    "# 3.the number of unique words and the number of uses of each word\n",
    "\n",
    "text = \"\"\"We first preprocess the data. We will give a summary of the most important parts,\\\n",
    "but please see our preprocessing code in our GitHub repo for more details.\\\n",
    "The Spotify dataset has 1 million playlists, over 2 million unique songs,\\\n",
    "and over 66 million edges linking playlists to songs. This is very large. We will cut\\\n",
    "down the dataset so that it can be trained on Colab in a reasonable amount of time.\\\n",
    "To do this, we will calculate the K-core of our graph. The K-core of a graph G is \\\n",
    "the largest possible connected subgraph of G, in which every node has a degree of at\\\n",
    "least K. This will give us the largest subgraph in which each playlist contains at least\\\n",
    "K songs, and each song is in at least K playlists.\\\n",
    "This has two benefits. Firstly, it will reduce the size of our dataset.\\\n",
    "Secondly, it will eliminate rare/unpopular songs, and small playlists.\\\n",
    "The remaining playlists/songs will all be relatively information-rich, which will \\\n",
    "make the learning process a bit easier, which is ideal for the purposes of this article.\"\"\"\n",
    "\n",
    "#Делаем все слова одного регистра и убираем все точки и запятиые,также убираем данный символ / между словами для верного подсчета.\n",
    "# и помещаем наш резултат в словарь(dictionary)\n",
    "unique = 0\n",
    "text = text.lower()\n",
    "#print (text)\n",
    "new_text=text.replace('/',',')\n",
    "new_text1=new_text.replace(',',' ')\n",
    "new_text2=new_text1.replace('.',' ')\n",
    "result = new_text2.split()\n",
    "unique = set(result)\n",
    "count = {}\n",
    "for i in result:\n",
    "    if i in count:\n",
    "        count[i]+=1\n",
    "    else:\n",
    "        count[i]=1\n",
    "print('The number of each word:',count)\n",
    "\n",
    "\n",
    "print ('The number of unique words:', len(unique))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# я пытаюсь посчитать уникальные слова.... которые встречаются один раз только\n",
    "#if count.get(i) == 1:\n",
    " #   unique +=1\n",
    "#else:\n",
    "#    print (unique)\n",
    "\n",
    "#print (count.get('data'))\n",
    "#print(list(count.keys())[list(count.values()).index(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9136f05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 177)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ВАРИАНТ ПРЕПОДАВАТЕЛЯ РЕШЕНИЯ ЗАДАЧИ НОМЕР 2:\n",
    "\n",
    "text = \"\"\"We first preprocess the data. We will give a summary of the most important parts,\\\n",
    "but please see our preprocessing code in our GitHub repo for more details.\\\n",
    "The Spotify dataset has 1 million playlists, over 2 million unique songs,\\\n",
    "and over 66 million edges linking playlists to songs. This is very large. We will cut\\\n",
    "down the dataset so that it can be trained on Colab in a reasonable amount of time.\\\n",
    "To do this, we will calculate the K-core of our graph. The K-core of a graph G is \\\n",
    "the largest possible connected subgraph of G, in which every node has a degree of at\\\n",
    "least K. This will give us the largest subgraph in which each playlist contains at least\\\n",
    "K songs, and each song is in at least K playlists.\\\n",
    "This has two benefits. Firstly, it will reduce the size of our dataset.\\\n",
    "Secondly, it will eliminate rare/unpopular songs, and small playlists.\\\n",
    "The remaining playlists/songs will all be relatively information-rich, which will \\\n",
    "make the learning process a bit easier, which is ideal for the purposes of this article.\"\"\"\n",
    "\n",
    "\n",
    "text = text.lower()\n",
    "tex = text.replace(',','')\n",
    "\n",
    "\n",
    "preds = text.split('.')\n",
    "len(preds) - 1\n",
    "# we should have 12 sentenses\n",
    "\n",
    "\n",
    "te = tex.replace('.','').replace('/',' ').replace('-',' ')\n",
    "te\n",
    "words = te.split(' ')\n",
    "len(words)\n",
    "voc = {}\n",
    "\n",
    "for word in words:\n",
    "    if word in voc:\n",
    "        voc[word]+=1\n",
    "    else:\n",
    "        voc[word]=1\n",
    "        \n",
    "len(voc),len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e329d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
